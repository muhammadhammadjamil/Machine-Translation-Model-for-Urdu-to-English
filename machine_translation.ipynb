{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import transformer\n",
        "import torch"
      ],
      "metadata": {
        "id": "H-eeb5t5iScY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9GYMEgPdh3MT"
      },
      "outputs": [],
      "source": [
        "from transformers import MarianMTModel, MarianTokenizer, TrainingArguments, Trainer\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "# Load pre-trained model and tokenizer\n",
        "model_name = \"Helsinki-NLP/opus-mt-ur-en\"\n",
        "model = MarianMTModel.from_pretrained(model_name)\n",
        "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Load sentences from files\n",
        "def load_sentences(file_path):\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "        sentences = file.read().splitlines()\n",
        "    return sentences\n",
        "\n",
        "urdu_sentences = load_sentences(\"/content/urdu-corpus.txt\")\n",
        "english_sentences = load_sentences(\"/content/english-corpus.txt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize and prepare data\n",
        "source_inputs = tokenizer(urdu_sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "target_inputs = tokenizer(english_sentences, padding=True, truncation=True, return_tensors=\"pt\")"
      ],
      "metadata": {
        "id": "btAV7Xb3iN2D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, source_inputs, target_inputs):\n",
        "        self.source_inputs = source_inputs\n",
        "        self.target_inputs = target_inputs\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.source_inputs[\"input_ids\"])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            \"input_ids\": self.source_inputs[\"input_ids\"][idx],\n",
        "            \"attention_mask\": self.source_inputs[\"attention_mask\"][idx],\n",
        "            \"labels\": self.target_inputs[\"input_ids\"][idx],\n",
        "        }"
      ],
      "metadata": {
        "id": "U_5eIkceiKEw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 16\n",
        "train_dataset = TranslationDataset(source_inputs, target_inputs)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "eSKZkd1yh7AH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine-tuning\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./output\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=5,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    save_steps=1000,\n",
        "    save_total_limit=2,\n",
        "    logging_dir=\"./logs\",\n",
        ")"
      ],
      "metadata": {
        "id": "lwWbd60rh8LQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "_U4a-VKzh-0n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_urdu_sentence = \"زین تمہارا بھتیجا ہے۔\""
      ],
      "metadata": {
        "id": "cKajaFXhi6jt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize input on the same device as your model (CPU or GPU)\n",
        "input_ids = tokenizer.encode(input_urdu_sentence, padding=True, truncation=True, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    # Move input to the same device as your model (CPU or GPU)\n",
        "    input_ids = input_ids.to(model.device)\n",
        "\n",
        "    # Generate translation on the same device as your model (CPU or GPU)\n",
        "    outputs = model.generate(input_ids)"
      ],
      "metadata": {
        "id": "Xz7pyrdQjMtN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Move generated output to the CPU for decoding and printing\n",
        "decoded_translation = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(decoded_translation)\n"
      ],
      "metadata": {
        "id": "NJxBag3ZjQih"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}